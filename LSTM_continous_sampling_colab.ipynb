{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_continous_sampling_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "A5QSJjAsUX7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# PyDrive reference:\n",
        "# https://googledrive.github.io/PyDrive/docs/build/html/index.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5MyOvtoRxJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#installing pytorch\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mreUEmuSurz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Auto-iterate through all files that matches this query\n",
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoJ16ZFyUyMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Auto-iterate through all files that matches this query\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1-XYATwD6zIEM-qhuf0_G6bbmTYIDn8D7' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pD4nC79DeIRK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# models fodler\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1pInhBXldeRAvBlphQvy5dh6iPE7DtYA2' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q4QP1hC5VffC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3. Load a file by ID and print its contents.\n",
        "downloaded = drive.CreateFile({'id': '1bZkY01oi6uojjmYr85kW1W9PLf9p5okR'})\n",
        "text = downloaded.GetContentString()\n",
        "print(text[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eZKpoTMfhqpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_to_drive_folder(folder_id, filename):\n",
        "  q_str = \"'{}' in parents and trashed=false\".format(folder_id)\n",
        "  file_list = drive.ListFile({'q': q_str}).GetList()\n",
        "  for file in file_list:\n",
        "    if file['title'] == filename:\n",
        "      found = True\n",
        "      print(\"file found in drive; overwriting\", end=\" \")\n",
        "      break\n",
        "  else:\n",
        "    found = False\n",
        "    print(\"file not found in drive; creating new file\")\n",
        "  \n",
        "  if not found:\n",
        "    file = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \n",
        "                                          \"id\": folder_id}]})#{'title': 'tiny_Shakespeare_LSTM_512.pkl'})\n",
        "  file.SetContentFile(filename)\n",
        "  file.Upload()\n",
        "  print('Uploaded file with ID {}'.format(file.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XoBSkcsVqOZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(1338)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-OQL41PVrBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2Tzg_i8Vyn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode_text(line,dict_):\n",
        "    vec = np.zeros((len(line),len(dict_)),dtype=\"uint8\")\n",
        "    for i, char in enumerate(line):\n",
        "        vec[i][dict_[char]] = 1\n",
        "    \n",
        "    return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4fQX5US_WbCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncodedCharDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Char based, one hot encoded dataset with continous sampling\"\"\"\n",
        "\n",
        "    def __init__(self, text, seq_len, char_to_idx={},idx_to_char={}):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            file: filepath\n",
        "            seq_len: lenght of sample the dataset will return\n",
        "        \"\"\"\n",
        "        self.offset = 0\n",
        "        \n",
        "        # length of sample sequences\n",
        "        self.seq_len = seq_len\n",
        "        \n",
        "        # char to id; Encoding dict\n",
        "        self.char_to_idx = char_to_idx\n",
        "        # id to char; Decoding dict\n",
        "        self.idx_to_char = idx_to_char\n",
        "        \n",
        "\n",
        "        self.data = np.empty(len(text))\n",
        "        # fill dicts\n",
        "        for i, char in enumerate(text): \n",
        "                if char not in self.char_to_idx:\n",
        "                    id_ = len(self.char_to_idx)\n",
        "                    # add char to dicts\n",
        "                    self.char_to_idx[char] = id_\n",
        "                    self.idx_to_char[id_] = char\n",
        "\n",
        "                # Encode and store text from file in self.data\n",
        "                self.data[i] = self.char_to_idx[char]\n",
        "\n",
        "\n",
        "        self.data = np.array(self.data,dtype=\"uint8\")\n",
        "        \n",
        "        self.unique_chars = len(self.char_to_idx)\n",
        "        print(\"#different chars:\", self.unique_chars)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return (len(self.data)) // self.seq_len #- (self.seq_len-1) #\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        returns (self.seq_len,self.unique_chars) one hot vectors \n",
        "        \"\"\"\n",
        "        indices = self.data[i*self.seq_len:(i+1)*self.seq_len] #[i:i+self.seq_len]#\n",
        "        x_onehot = torch.zeros((self.seq_len,self.unique_chars))\n",
        "        x_onehot[np.arange(self.seq_len), indices] = 1\n",
        "        \n",
        "        inputs = x_onehot[:-1,:]\n",
        "        targets = torch.argmax(x_onehot[1:,:],dim=-1)\n",
        "        return inputs, targets\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGBPJlknXxSF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_size = 2000\n",
        "\n",
        "val_text = text[-val_size:]\n",
        "test_text = text[:-val_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XTAUjyKX4GD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_len = 129\n",
        "batch_size = 1\n",
        "kwargs = {'num_workers': 1, \n",
        "          'pin_memory': True} if torch.cuda.is_available() else {'num_workers': 2}\n",
        "train_dataset = EncodedCharDataset(text=test_text,seq_len=seq_len)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=seq_len,\n",
        "                                           shuffle=True, **kwargs)\n",
        "\n",
        "val_dataset = EncodedCharDataset(text=val_text,seq_len=len(val_text),\n",
        "                                 char_to_idx=train_dataset.char_to_idx,\n",
        "                                 idx_to_char=train_dataset.idx_to_char)\n",
        "#val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1,\n",
        "#                                           shuffle=False, **kwargs)\n",
        "\n",
        "assert(train_dataset.unique_chars == val_dataset.unique_chars)\n",
        "num_classes = train_dataset.unique_chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2w0E9wqvYSrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharLSTM(torch.nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size, num_layers=1,\n",
        "                 dropout=0,batch_first=True):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.GRU(input_size, hidden_size, \n",
        "                           num_layers=num_layers,\n",
        "                           dropout=dropout,\n",
        "                           batch_first=batch_first)\n",
        "        self.lin = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, inputs, hidden=None, force=False, steps=0):\n",
        "        output, hidden = self.lstm(inputs, hidden)\n",
        "        output = self.lin(output)\n",
        "\n",
        "        return output, hidden\n",
        "      \n",
        "    def init_hidden(self, batch_size=1):\n",
        "      return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJwJ0fBVYmcr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_text(model, init_seq=\"There is\", temperature=.9,\n",
        "               lenght=250,\n",
        "               idx_to_char=train_dataset.idx_to_char, \n",
        "               char_to_idx=train_dataset.char_to_idx):\n",
        "    \"\"\"\n",
        "    generates text of length lenght from model using start_char to initialize hidden state and \n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      hidden = model.init_hidden()\n",
        "      \n",
        "      if torch.cuda.is_available():\n",
        "          hidden = hidden.cuda()\n",
        "      \n",
        "      # initialize hidden state with start_char seqence\n",
        "      for chr in init_seq:\n",
        "          # encode the initialization sequence\n",
        "          input = torch.zeros((1,1,num_classes),dtype=torch.float)\n",
        "          input[0][0][char_to_idx[chr]] = 1 \n",
        "          \n",
        "          if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            \n",
        "          output,hidden = model(input,hidden)\n",
        "      \n",
        "      # generate new seqence \n",
        "      text = \"\"\n",
        "      while len(text) < lenght:\n",
        "          # prob output to one hot encoded vector\n",
        "          probs = nn.Softmax(dim=-1)(output).detach().cpu().numpy()[0,0]\n",
        "          \n",
        "          # choose next char\n",
        "          char_id = sample(probs,temperature)#output.argmax(dim=-1).item()\n",
        "          \n",
        "          # one-hot encode new char for new input to model \n",
        "          input = torch.zeros((1,1,num_classes),dtype=torch.float)\n",
        "          input[0][0][char_id] = 1 \n",
        "          \n",
        "          if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "          \n",
        "          output, hidden = model(input,hidden)\n",
        "          \n",
        "          # decode generated char and add to string\n",
        "          text += idx_to_char[char_id]\n",
        "    return text\n",
        "\n",
        "def sample(a, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array a\n",
        "    a = np.log(a) / temperature\n",
        "    a = np.exp(a) / np.sum(np.exp(a))\n",
        "    return np.random.choice(range(num_classes),p=a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aR7TPOEkaKht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inputs,targets):\n",
        "  \"\"\"\n",
        "  trains the model step by step for a batch of inputs of shape \n",
        "  (batch_size, seq_len, num_classes) and targets of shape ******\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  loss = 0\n",
        "  hidden = model.init_hidden(batch_size=inputs.shape[0])\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "      inputs, targets, hidden = inputs.cuda(), targets.cuda(), hidden.cuda()\n",
        "\n",
        "  for i in range(inputs.shape[1]):\n",
        "    out,hidden = model(inputs[:,i:i+1,:],hidden)\n",
        "    loss += criterion(out.permute(dims=(0,2,1)), targets[:,i:i+1])\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item() / inputs.shape[1] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vg5l7vmda2jC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(inputs, targets):\n",
        "    \"\"\"\n",
        "    evaluates the model step by step for a batch of inputs of shape \n",
        "    (seq_len, num_classes) and targets of shape *****\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "      inputs = inputs.expand(1,-1,-1)\n",
        "      #print(inputs.shape)\n",
        "      targets = targets.expand(1,-1)\n",
        "      #print(targets.shape)\n",
        "      hidden = model.init_hidden(batch_size=inputs.shape[0])\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          inputs, targets, hidden = inputs.cuda(), targets.cuda(), hidden.cuda()\n",
        "\n",
        "      for i in range(inputs.shape[1]):\n",
        "        out,hidden = model(inputs[:,i:i+1,:], hidden)\n",
        "        loss += criterion(out.permute(dims=(0,2,1)), targets[:,i:i+1])\n",
        "\n",
        "    return loss.item() / inputs.shape[1] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5lFzLXXScfR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    print(\"saving model\")\n",
        "    torch.save(state, filename)\n",
        "      \n",
        "    #if is_best:\n",
        "    #    shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "    \n",
        "def load_checkpoint(filename):\n",
        "    if os.path.isfile(filename):\n",
        "        global start_epoch\n",
        "        global best_val\n",
        "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "        checkpoint = torch.load(filename)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_val = checkpoint['best_val']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler = checkpoint['scheduler']\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "              .format(filename, checkpoint['epoch']))\n",
        "    else:\n",
        "        print(\"=> no checkpoint found at '{}'\".format(filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-SE_Ku5xYn26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_size = num_classes\n",
        "hidden_size = 256\n",
        "output_size = num_classes\n",
        "num_layers = 2\n",
        "dropout = 0.15\n",
        "filename = 'shakesbeare_checkpoint.pth.tar'\n",
        "\n",
        "model = CharLSTM(input_size=input_size,hidden_size=hidden_size,\n",
        "                 output_size=output_size,num_layers=num_layers,\n",
        "                 dropout=dropout,batch_first=True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fqgfoh3TZdDb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start_epoch = 1\n",
        "best_val = 1000\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min',\n",
        "                                                 verbose=True)\n",
        "\n",
        "\n",
        "losses = []\n",
        "mean_losses = []\n",
        "val_losses = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IqPlWvI4nI40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model from checkpoint\n",
        "# TODO: load directly from google drive\n",
        "load_checkpoint(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzdO7g4HZeZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training LSTM\n",
        "for epoch in range(start_epoch, 300):\n",
        "    print(\"\\nEpoch {}:\".format(epoch))\n",
        "    # train epoch\n",
        "    losses.append([])\n",
        "    for batch, (inputs, targets) in enumerate(train_loader):\n",
        "        losses[-1].append(train(inputs, targets))\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            rmean_loss = np.mean(losses[-1][-15:])\n",
        "            print(\"\\rLoss: {:.5f} | Running loss: {:.5f}, Batch: {}       \"\n",
        "                  .format(losses[-1][-1],rmean_loss,batch), end=\"\")\n",
        "        #if batch % 200 == 0: \n",
        "        #  print(gen_text(model))\n",
        "        #  model.train()\n",
        "    \n",
        "    mean_loss = np.mean(losses[-1])\n",
        "    mean_losses.append(mean_loss)\n",
        "    print(\"\\nMean Loss:\", mean_loss)\n",
        "    \n",
        "    # validate    \n",
        "    val_loss = evaluate(*val_dataset[0])\n",
        "    val_losses.append(val_loss)\n",
        "    print(\"\\nValidation Loss: {:.5f} | Prev best: {:.5f}\".format(val_loss, best_val))\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Plot training progress / batchwise train error and val_loss\n",
        "    fig = plt.figure()\n",
        "    ax = plt.axes()\n",
        "    # flatten losses array\n",
        "    flat_losses = [item for sublist in losses for item in sublist]\n",
        "    # get corresponding x-axis positions for the val error\n",
        "    val_losses_x = []\n",
        "    x = 0\n",
        "    for sublist in losses:\n",
        "      x += len(sublist)\n",
        "      val_losses_x.append(x)\n",
        "    #val_losses_x = [len(sublist) for sublist in losses]\n",
        "    ax.plot(range(len(flat_losses)), flat_losses)\n",
        "    ax.plot(val_losses_x, val_losses)\n",
        "    plt.show()\n",
        "    \n",
        "    # Model saving\n",
        "    is_best = val_loss < best_val\n",
        "    if is_best:\n",
        "        # save model if validation error is minimal\n",
        "        best_val = val_loss\n",
        "        save_checkpoint({\n",
        "                'epoch':  epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_val': val_loss,\n",
        "                'optimizer' : optimizer.state_dict(),\n",
        "                'scheduler': scheduler,\n",
        "            }, is_best, filename)\n",
        "    \n",
        "    # save model to google drive \n",
        "    save_to_drive_folder('1pInhBXldeRAvBlphQvy5dh6iPE7DtYA2',filename)\n",
        "    \n",
        "    if epoch % 10 == 0 and epoch > 0:\n",
        "        # show some example by the curren model\n",
        "        print(\"Samples with increasing Temp -> conservative first:\")\n",
        "        for i in range(5):\n",
        "            temp = 0.2*(i+1)\n",
        "            print(\"Temprature: \", temp)\n",
        "            print(gen_text(model,temperature=temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGM1rQogdPAb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IRDpjFb0ebt4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.listdir(\".\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Spv_MmqlefCB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_checkpoint('shakesbeare_checkpoint.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kcp-fu1ZmAk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}